{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Feb 9 2025\n",
    "\n",
    "@author: Yaning\n",
    "\"\"\"\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"dbmdz/german-gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open('C:/Users/Administrator/Desktop/llm/winnetou_s.txt', 'r') as file:\n",
    "    text_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the input text exceeds the max_length, split it into smaller chunks\n",
    "def split_text_into_chunks(text, chunk_size):\n",
    "    # Tokenize the entire text and split it into smaller chunks\n",
    "    tokenized_text = tokenizer(text, padding=False, truncation=False, return_tensors=\"pt\")\n",
    "    total_tokens = tokenized_text['input_ids'][0]\n",
    "    chunks = [total_tokens[i:i + chunk_size] for i in range(0, len(total_tokens), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Split into chunks (if necessary)\n",
    "chunks = split_text_into_chunks(text_lines[0], max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for single line split into chunks\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.chunks[idx]),\n",
    "            \"attention_mask\": torch.ones_like(self.chunks[idx]),  # Attention mask for the whole sequence\n",
    "            \"labels\": torch.tensor(torch.cat((chunks[0][1:],torch.tensor([1])), dim=0))\n",
    "        }\n",
    "\n",
    "# Create a dataset instance\n",
    "dataset = TextDataset(chunks)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA settings\n",
    "lora_config = LoraConfig(\n",
    "    r=20,  # Rank of the low-rank matrices (can experiment with different values)\n",
    "    lora_alpha=3,  # Scaling factor\n",
    "    lora_dropout=0.3,  # Dropout rate for LoRA layers\n",
    "    task_type=\"CAUSAL_LM\"  # Causal Language Model task\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Use the default data collator for language modeling tasks (this will handle padding)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Since GPT-2 is a causal language model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./german-gpt2-lora\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",  # Log at intervals\n",
    "    logging_steps=1,  # Log loss every 10 steps\n",
    "    # report_to=\"tensorboard\",\n",
    "    # save_steps=10_000,\n",
    "    # save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Assuming you have already set up the LoRA model (lora_model)\n",
    "trainer = Trainer(\n",
    "    model=lora_model,  # Your LoRA model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Pass the dataset here\n",
    "    eval_dataset = eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./german-gpt2-lora/full\\\\tokenizer_config.json',\n",
       " './german-gpt2-lora/full\\\\special_tokens_map.json',\n",
       " './german-gpt2-lora/full\\\\vocab.json',\n",
       " './german-gpt2-lora/full\\\\merges.txt',\n",
       " './german-gpt2-lora/full\\\\added_tokens.json',\n",
       " './german-gpt2-lora/full\\\\tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./german-gpt2-lora/full')\n",
    "tokenizer.save_pretrained('./german-gpt2-lora/full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./german-gpt2-lora/full were not used when initializing GPT2LMHeadModel: ['transformer.h.0.attn.c_attn.base_layer.bias', 'transformer.h.0.attn.c_attn.base_layer.weight', 'transformer.h.0.attn.c_attn.lora_A.default.weight', 'transformer.h.0.attn.c_attn.lora_B.default.weight', 'transformer.h.1.attn.c_attn.base_layer.bias', 'transformer.h.1.attn.c_attn.base_layer.weight', 'transformer.h.1.attn.c_attn.lora_A.default.weight', 'transformer.h.1.attn.c_attn.lora_B.default.weight', 'transformer.h.10.attn.c_attn.base_layer.bias', 'transformer.h.10.attn.c_attn.base_layer.weight', 'transformer.h.10.attn.c_attn.lora_A.default.weight', 'transformer.h.10.attn.c_attn.lora_B.default.weight', 'transformer.h.11.attn.c_attn.base_layer.bias', 'transformer.h.11.attn.c_attn.base_layer.weight', 'transformer.h.11.attn.c_attn.lora_A.default.weight', 'transformer.h.11.attn.c_attn.lora_B.default.weight', 'transformer.h.2.attn.c_attn.base_layer.bias', 'transformer.h.2.attn.c_attn.base_layer.weight', 'transformer.h.2.attn.c_attn.lora_A.default.weight', 'transformer.h.2.attn.c_attn.lora_B.default.weight', 'transformer.h.3.attn.c_attn.base_layer.bias', 'transformer.h.3.attn.c_attn.base_layer.weight', 'transformer.h.3.attn.c_attn.lora_A.default.weight', 'transformer.h.3.attn.c_attn.lora_B.default.weight', 'transformer.h.4.attn.c_attn.base_layer.bias', 'transformer.h.4.attn.c_attn.base_layer.weight', 'transformer.h.4.attn.c_attn.lora_A.default.weight', 'transformer.h.4.attn.c_attn.lora_B.default.weight', 'transformer.h.5.attn.c_attn.base_layer.bias', 'transformer.h.5.attn.c_attn.base_layer.weight', 'transformer.h.5.attn.c_attn.lora_A.default.weight', 'transformer.h.5.attn.c_attn.lora_B.default.weight', 'transformer.h.6.attn.c_attn.base_layer.bias', 'transformer.h.6.attn.c_attn.base_layer.weight', 'transformer.h.6.attn.c_attn.lora_A.default.weight', 'transformer.h.6.attn.c_attn.lora_B.default.weight', 'transformer.h.7.attn.c_attn.base_layer.bias', 'transformer.h.7.attn.c_attn.base_layer.weight', 'transformer.h.7.attn.c_attn.lora_A.default.weight', 'transformer.h.7.attn.c_attn.lora_B.default.weight', 'transformer.h.8.attn.c_attn.base_layer.bias', 'transformer.h.8.attn.c_attn.base_layer.weight', 'transformer.h.8.attn.c_attn.lora_A.default.weight', 'transformer.h.8.attn.c_attn.lora_B.default.weight', 'transformer.h.9.attn.c_attn.base_layer.bias', 'transformer.h.9.attn.c_attn.base_layer.weight', 'transformer.h.9.attn.c_attn.lora_A.default.weight', 'transformer.h.9.attn.c_attn.lora_B.default.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at ./german-gpt2-lora/full and are newly initialized: ['transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Replace with the directory where your model is saved\n",
    "fine_tuned_model_dir = \"./german-gpt2-lora/full\"  # Adjust this if needed\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"ÜbersetzÜbersetzen Sie die folgenden Sätze in den sächsischen Dialekt: Unser Team besteht aus mehr als 180 Personen, darunter renommierte internationale Forscher sowie hochqualifizierte Fachkräfte in administrativen und kommunikativen Rollen. Mit mehr als 60 leitenden Forschern, zwei Humboldt-Professuren und bis zu zwölf geplanten KI-Professuren unterstützen wir Exzellenz in der Forschung und Lehre in Leipzig und Dresden. Die Förderung von jungen Talenten ist ebenfalls ein wichtiger Bestandteil unserer Arbeit, weshalb wir vier Junior-Forschungsgruppen gegründet haben, die unsere aktuellen Forschungsthemen sinnvoll ergänzen. Darüber hinaus begrüßen wir assoziierte Mitglieder, die ihr Fachwissen in unser Zentrum einbringen.\"\n",
    "# input_text = \"Was ist die Liebe\"\n",
    "# input_text = \"Schreiben Sie eine Kurzgeschichte über zwei Sachsen bei der Deutschen Bahn\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÜbersetzÜbersetzen Sie die folgenden Sätze in den sächsischen Dialekt: Unser Team besteht aus mehr als 180 Personen, darunter renommierte internationale Forscher sowie hochqualifizierte Fachkräfte in administrativen und kommunikativen Rollen. Mit mehr als 60 leitenden Forschern, zwei Humboldt-Professuren und bis zu zwölf geplanten KI-Professuren unterstützen wir Exzellenz in der Forschung und Lehre in Leipzig und Dresden. Die Förderung von jungen Talenten ist ebenfalls ein wichtiger Bestandteil unserer Arbeit, weshalb wir vier Junior-Forschungsgruppen gegründet haben, die unsere aktuellen Forschungsthemen sinnvoll ergänzen. Darüber hinaus begrüßen wir assoziierte Mitglieder, die ihr Fachwissen in unser Zentrum einbringen. Die Forschungsgruppe ist in der Lage, auf den neuesten Stand der Forschung zu kommen, weil sie sich spezialisiert hat, um in ihrer Arbeit nicht zu stark mit den Universitäten verbunden zu sein, die an der Universität in der Wissenschaft nicht vertreten sind. Das ist eine gute Grundlage, um neue Erkenntnisse zu entdecken und zu bearbeiten. So ist es uns gelungen, die Forschung in der Wissenschaft zu fördern und zu koordinieren. Es macht uns stolz, dass wir so viele junge Forscher in unsere Forschungsgruppe aufnehmen können. Es wird immer wieder von den Kollegen und von den Studenten der Universität Leipzig unterstützt. Es ist uns ein besonderes Anliegen, dass die Forscher bei der Arbeit an den Hochschulen und\n"
     ]
    }
   ],
   "source": [
    "# Set generation parameters (you can customize these)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=255,       # Maximum length of the generated text\n",
    "    num_return_sequences=1,  # Number of sequences to return\n",
    "    temperature=0.7,      # Control randomness in generation\n",
    "    top_k=50,             # Limit to the top_k most likely next tokens\n",
    "    top_p=0.95,           # Nucleus sampling (only use top_p probability mass)\n",
    "    do_sample=True,       # Whether to use sampling (True) or greedy decoding (False)\n",
    ")\n",
    "\n",
    "# Decode the generated output back to text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
