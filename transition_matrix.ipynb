{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Feb 9 2025\n",
    "\n",
    "@author: Yaning\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torch.nn.functional as F # mainly for ReLU\n",
    "import numpy as np\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|████████████████████████████████████████████████████████████| 4/4 [06:21<00:00, 95.48s/it]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaning/Documents/python_env/llm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50265, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "# model = GPT2Model.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text = \"Haid geh ich in die Schdadt, um paar Eingaufn zu machn. Dann treffsch mich mit Feiern im Café un genießn e heeßn Gaffee. Es is'n schieener Tach, un ich freu mich offn Oobnd.\"\n",
    "h_text = \"Heute gehe ich in die Stadt, um ein paar Einkäufe zu machen. Danach treffe ich mich mit Freunden im Café und genieße einen heißen Kaffee. Es ist ein schöner Tag, und ich freue mich auf den Abend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_s_text = s_text.lower()\n",
    "processed_s_text = re.sub(r'[.,]', \" \", processed_s_text)\n",
    "processed_s_text = re.sub(r'\\s+', \" \", processed_s_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_h_text = h_text.lower()\n",
    "processed_h_text = re.sub(r'[.,]', \" \", processed_h_text)\n",
    "processed_h_text = re.sub(r'\\s+', \" \", processed_h_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heute gehe ich in die stadt um ein paar einkäufe zu machen danach treffe ich mich mit freunden im café und genieße einen heißen kaffee es ist ein schöner tag und ich freue mich auf den abend '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_h_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h = processed_h_text[:16]\n",
    "test_s = processed_s_text[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heute gehe ich in die stadt um ein paar einkäufe zu machen danach treffe ich mich mit freunden im café und genieße einen heißen kaffee es ist ein schöner tag und ich freue mich auf den abend '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_h_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heute gehe ich i'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yahu202d/workspaces/horse/yahu202d-saexy/data/hochdeutsch.txt', 'r') as file:\n",
    "    h_text = file.read()\n",
    "\n",
    "with open('/home/yahu202d/workspaces/horse/yahu202d-saexy/data/sachsen.txt', 'r') as file:\n",
    "    s_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transition Layer (T)\n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        # Trainable transition matrix to map standard to dialect\n",
    "        self.transition_matrix = nn.Parameter(torch.randn(embedding_dim, embedding_dim))\n",
    "    \n",
    "    def forward(self, standard_embeddings):\n",
    "        # Apply the transformation: H_dialect = T * H_standard\n",
    "        return torch.matmul(standard_embeddings, self.transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = model.config.hidden_size  # LLaMA hidden size (usually 4096 for LLaMA-7B)\n",
    "transition_layer = TransitionLayer(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransitionLayer()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move everything to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "transition_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Guten Morgen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_h = tokenizer(processed_h_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens_s = tokenizer(processed_s_text, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_s_first_20 = BatchEncoding({\n",
    "    'input_ids': tokens_s['input_ids'][:, :20],\n",
    "    'attention_mask': tokens_s['attention_mask'][:, :20]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_h_first_20 = BatchEncoding({\n",
    "    'input_ids': tokens_h['input_ids'][:, :20],\n",
    "    'attention_mask': tokens_h['attention_mask'][:, :20]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel) (transition_matrix.ipynb)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for ms-toolsai.jupyter:_builtin.jupyterServerUrlProvider:ce06527e-47d3-4b9b-a18f-ad3891457f8a"
     ]
    }
   ],
   "source": [
    "tokens_h_first_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Example parallel dataset (Standard ↔ Dialect)\n",
    "standard_sentences = [\n",
    "    \"Haid geh ich in die Schdadt\",\n",
    "    \"um paar Eingaufn zu machn\",\n",
    "    \"Dann treffsch mich mit Feiern im Café un genießn e heeßn Gaffee\"\n",
    "]\n",
    "\n",
    "dialect_sentences = [\n",
    "    \"Heute gehe ich in die Stadt\",\n",
    "    \"um ein paar Einkäufe zu machen\",\n",
    "    \"Danach treffe ich mich mit Freunden im Café und genieße einen heißen Kaffee\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "def tokenize_sentences(sentences):\n",
    "    return tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "standard_inputs = tokenize_sentences(standard_sentences)\n",
    "dialect_inputs = tokenize_sentences(dialect_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 2566.3745\n",
      "Epoch [4/100], Loss: 2495.0471\n",
      "Epoch [6/100], Loss: 2425.3965\n",
      "Epoch [8/100], Loss: 2357.4358\n",
      "Epoch [10/100], Loss: 2291.1714\n",
      "Epoch [12/100], Loss: 2226.6003\n",
      "Epoch [14/100], Loss: 2163.7163\n",
      "Epoch [16/100], Loss: 2102.5088\n",
      "Epoch [18/100], Loss: 2042.9645\n",
      "Epoch [20/100], Loss: 1985.0653\n",
      "Epoch [22/100], Loss: 1928.7887\n",
      "Epoch [24/100], Loss: 1874.1079\n",
      "Epoch [26/100], Loss: 1820.9930\n",
      "Epoch [28/100], Loss: 1769.4128\n",
      "Epoch [30/100], Loss: 1719.3331\n",
      "Epoch [32/100], Loss: 1670.7191\n",
      "Epoch [34/100], Loss: 1623.5348\n",
      "Epoch [36/100], Loss: 1577.7441\n",
      "Epoch [38/100], Loss: 1533.3096\n",
      "Epoch [40/100], Loss: 1490.1947\n",
      "Epoch [42/100], Loss: 1448.3630\n",
      "Epoch [44/100], Loss: 1407.7780\n",
      "Epoch [46/100], Loss: 1368.4037\n",
      "Epoch [48/100], Loss: 1330.2047\n",
      "Epoch [50/100], Loss: 1293.1460\n",
      "Epoch [52/100], Loss: 1257.1937\n",
      "Epoch [54/100], Loss: 1222.3137\n",
      "Epoch [56/100], Loss: 1188.4736\n",
      "Epoch [58/100], Loss: 1155.6409\n",
      "Epoch [60/100], Loss: 1123.7842\n",
      "Epoch [62/100], Loss: 1092.8724\n",
      "Epoch [64/100], Loss: 1062.8762\n",
      "Epoch [66/100], Loss: 1033.7661\n",
      "Epoch [68/100], Loss: 1005.5138\n",
      "Epoch [70/100], Loss: 978.0921\n",
      "Epoch [72/100], Loss: 951.4747\n",
      "Epoch [74/100], Loss: 925.6359\n",
      "Epoch [76/100], Loss: 900.5511\n",
      "Epoch [78/100], Loss: 876.1966\n",
      "Epoch [80/100], Loss: 852.5494\n",
      "Epoch [82/100], Loss: 829.5875\n",
      "Epoch [84/100], Loss: 807.2894\n",
      "Epoch [86/100], Loss: 785.6344\n",
      "Epoch [88/100], Loss: 764.6028\n",
      "Epoch [90/100], Loss: 744.1753\n",
      "Epoch [92/100], Loss: 724.3334\n",
      "Epoch [94/100], Loss: 705.0591\n",
      "Epoch [96/100], Loss: 686.3353\n",
      "Epoch [98/100], Loss: 668.1453\n",
      "Epoch [100/100], Loss: 650.4730\n",
      "Training complete. Transition matrix learned.\n"
     ]
    }
   ],
   "source": [
    "# Define Mean Squared Error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(transition_layer.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get standard language embeddings\n",
    "    with torch.no_grad():\n",
    "        standard_outputs = model(**tokens_h_first_20.to(device))\n",
    "        # standard_outputs = model.base_model(**standard_inputs.to(device), output_hidden_states=True)\n",
    "        # standard_hidden_states = standard_outputs.hidden_states[-1]  # Last hidden state (batch_size, seq_len, hidden_size)\n",
    "        standard_hidden_states = standard_outputs.last_hidden_state\n",
    "\n",
    "    # Get dialect embeddings (ground truth)\n",
    "    with torch.no_grad():\n",
    "        dialect_outputs = model(**tokens_s_first_20.to(device))\n",
    "        dialect_hidden_states = dialect_outputs.last_hidden_state\n",
    "    \n",
    "    # Apply the transition matrix to map standard to dialect space\n",
    "    predicted_dialect_hidden_states = transition_layer(standard_hidden_states)\n",
    "    \n",
    "    # Compute the loss between transformed embeddings and ground truth dialect embeddings\n",
    "    loss = criterion(predicted_dialect_hidden_states, dialect_hidden_states)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, the transition layer will contain the trained matrix\n",
    "print(\"Training complete. Transition matrix learned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"dbmdz/german-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: was ist die liebe\n",
      "Dialect Response: was ist die liebeQuestion born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "Question born das beste\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After training, use the trained transition matrix to generate dialect responses\n",
    "max_length = 10\n",
    "def generate_dialect_response(input_text):\n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    # Get standard language embeddings\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(**inputs.to(\"cpu\"), output_hidden_states=True)\n",
    "            standard_hidden_states = outputs.hidden_states[-1]  # Last hidden state\n",
    "    \n",
    "            # Apply the transition matrix to convert standard to dialect\n",
    "            # dialect_hidden_states = transition_layer(standard_hidden_states)\n",
    "            dialect_hidden_states = standard_hidden_states\n",
    "\n",
    "    \n",
    "            # Use transformed dialect embeddings to generate text\n",
    "            outputs.logits = model.lm_head(dialect_hidden_states)\n",
    "    \n",
    "            # Generate text from the transformed hidden states (using argmax or beam search)\n",
    "            generated_tokens = torch.argmax(outputs.logits, dim=-1)\n",
    "            input_ids = torch.cat([input_ids, generated_tokens], dim=-1)\n",
    "\n",
    "\n",
    "    response = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example question\n",
    "input_question = \"was ist die liebe\"\n",
    "dialect_response = generate_dialect_response(input_question)\n",
    "print(f\"Input: {input_question}\")\n",
    "print(f\"Dialect Response: {dialect_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_embeddings(input_text, model, tokenizer, max_length=50, device=\"cpu\"):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Store embeddings of all generated tokens\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Pass input_ids through the model to get logits and hidden states\n",
    "            outputs = model(input_ids=input_ids, output_hidden_states=True)\n",
    "            \n",
    "            # Get the last hidden state (embeddings for the most recent token)\n",
    "            hidden_state = outputs.hidden_states[-1][:, -1, :]  # Shape: (batch_size, embedding_dim)\n",
    "            all_embeddings.append(hidden_state)  # Save the embedding\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Generate the next token using greedy decoding (argmax)\n",
    "            next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Append the new token to input_ids for the next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "            \n",
    "            # Optional: Stop if end-of-sequence token is generated\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the final sequence of tokens\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text, torch.cat(all_embeddings, dim=0)  # Return the generated text and embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: was ist die liebe\n",
      "Dialect Response: ('was ist die liebe\\nwas ist die liebe\\nPost by 1st » 2019-09-11 08:00\\nwas ist die liebe\\nPost by 1st » 2019-09-11 08:00\\nwas ist die', tensor([[-0.3350,  1.3654,  3.1655,  ...,  0.6161,  0.4388,  0.4800],\n",
      "        [ 0.8461,  2.3800,  2.1139,  ..., -0.0284,  0.4820,  0.6537],\n",
      "        [ 1.8327,  0.3759,  1.4126,  ...,  0.6590,  0.6365,  2.0774],\n",
      "        ...,\n",
      "        [ 0.8370,  2.4210,  2.0880,  ..., -1.1345, -0.2371,  0.3876],\n",
      "        [ 1.4597,  1.0331, -0.0953,  ...,  1.2783,  0.6289,  1.7850],\n",
      "        [ 0.8383,  1.6625,  1.0387,  ..., -0.4015,  0.3411,  0.7449]]))\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "input_question = \"was ist die liebe\"\n",
    "dialect_response = generate_with_embeddings(input_question,model,tokenizer)\n",
    "print(f\"Input: {input_question}\")\n",
    "print(f\"Dialect Response: {dialect_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA's Response: Was ist die liebekraft? - Was\n"
     ]
    }
   ],
   "source": [
    "def ask_llama(question, model, tokenizer, max_length=10, device=\"cpu\"):\n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
    "            top_k=50,         # Top-k sampling for diversity\n",
    "            do_sample=True    # Enable sampling for less repetitive responses\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response, output[0]\n",
    "\n",
    "# Example: Ask a question\n",
    "question = \"Was ist die liebekraft?\"\n",
    "response, output = ask_llama(question, model, tokenizer)\n",
    "print(\"LLaMA's Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,  27125,   6127,   2815,  10457,  77614,   3017,     30,    482,\n",
       "         15148])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"diese übersetzung ist sehr unterschiedlich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = tokenizer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bek'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(77614)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
